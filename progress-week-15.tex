\section{Introduction}
I have spent the majority of my time this week trying my LSTM with others features extract with multiple methods and re-implementing the extracting features part of TensorFlow for Poets to build up the full network version (Extracting Feature and LSTM in a single network)

\section{PyTorch LSTM Classification Network}
So far I had tried many different methods to extract features and used them as input for my LSTM network, but I found that the method that providing the best features for my LSTM network was the pretrained Inception V3 model from 2015 by Google TensorFlow for Poets.

This week I tried normalizing the images before extracting their features with PyTorch's pretrained models. But the results were just the same as TensorHub, they all could not be better than the TensorFlow for Poets' features.

\section{Full Network}
\subsection{Feature Extracting Layer}
Based on the example code by TensorFlow for Poets, I implemented my own version.

\textbf{\emph{Step 1}}. Retrieving Inception V3 model detail information such as where to download it, bottleneck layer's name, size of extracting features, input images' size required by model, mean and standard deviation for normalizing input images.

\textbf{\emph{Step 2}}. Using the model link provide at step 1 to download and extract the model on disk.

\textbf{\emph{Step 3}}. Generating default TensorFlow Graph\cite{tfgraph}, grasping the tensors of resizing input images, computing bottleneck values from the model downloaded at step 2. Then I had to add both tensors above to the generated Graph for further usages.

\textbf{\emph{Step 4}}. Creating tensor to decode input images from string type to float types and another tensor to reshape and normalize decoded images from the size information got from step 1. Then I had to add both tensors above to the generated Graph for further usages.

\textbf{\emph{Step 5}}. Examining all input videos, caching their bottleneck values on disk and loading them into training dataset at the same time.

\subsection{Long-Short Term Memory Layer}
This Long-Short Term Memory layer was backed by Keras, took in the feature vectors from first part above and outputted a sequence of 8 vectors (for each video).

The input size of this layer was equivalent to the size of the feature vectors (of length 2048) for each video which was totally [8, 2048], the drop out indicating fraction of the units to drop for the linear transformation of the inputs was 0.5, the hidden size indicating dimensionality of the output space was set to be 1024 or 2048 based on preference. The output size of this layer would be [8, 1024] or [8, 2048] based on how I configed the hidden size. One more notable thing was that I only used one LSTM layer which meant there was no stacking here.

\subsection{Time-Distributed Layer}
This Time-Distributed layer was also backed by Keras, took in the output sequence of vectors from LSTM layer and outputted the memorability score of a video.

The input size of this layer was [8, 1024] or [8, 2048] based on the previous layer. This layer applied the Dense layer to each timestep independently, outputted a vector of size [8, 1] for each input indicating the memorability score through out every timestep (every frame).

The last layer attached to this Time-Distributed layer was Lambda layer. Its purpose was to extract only the last memorability score (from [8, 1] to [1, 1]) of the Time-Distributed's output.

\subsection{Cost Function and Optimizer}
The official cost function of this model was Mean-Squared Error, and the offical optimizer was Adam optimizer with default learning-rate of 1e-3.

\section{Results}

After training, 5 checkpoints were selected for each sub-task (short-term and long-term).

\textbf{For short-term subtask}, we chose 2 Time-Distributed checkpoints each with 2048 hidden units, 1 Time-Distributed checkpoints with 1024 hidden units, and 2 AMNet checkpoints on split 0 and split 4. The table below demonstrated the checkpoints' names and their train, test correlation. The term \emph{TD} indicated Time-Distributed, \emph{AMNet} indicated AMNet model. The second terms indicated additional information about the checkpoints (hidden units sizes for Time-Distributed and split number for AMNet).

\begin{table}[h]
\begin{center}
\begin{tabular}{@{}lcc@{}}
Checkpoint & Train Correl & Test Correl \\ \midrule
TD-2048 & 0.5325 & 0.479 \\
TD-1024 & 0.5105 & 0.484 \\
AMNet-split0 & 0.480 & 0.447 \\
AMNet-split1 & 0.487 & 0.455 \\
TD-0.5246-2048 & 0.5246 & 0.478
\end{tabular}
\caption{Short-term Spearman's Rank Correlation (Official Test Set)}
\end{center}
\end{table}

The results are pretty much what we expected initially. Time distributed models with 2048 units turned out to have slightly worse performance than the one with 1024 units. This may due to the fact that the model is more complicated and we did chose these models after 40th epoch, where overfitting usually occurs.

\textbf{For long-term subtask}, we chose 3 different checkpoints, 1 Time-Dis-tributed with 1024 hidden units being trained on long-term data, another Time-Distributed (also have 1024 hidden units) being trained on short-term data, 1 AMNet model. For the other two runs, we reused two pre-trained model purely on short-term data (from the short-term subtask). The table below demonstrated the checkpoints' names and their train, test correlation. The term \emph{TD} indicated Time-Distributed, \emph{AMNet} indicated AMNet model. The second terms indicated additional information about the checkpoints (hidden units sizes for Time-Distributed and split number for AMNet). And the terms \emph{byLong} or \emph{byShort} indicated which dataset the checkpoints were being trained with.

\begin{table}[h]
\begin{center}
\begin{tabular}{@{}lcc@{}}
Checkpoint & Train Correl & Test Correl \\ \midrule
TD-byLong-1024 & 0.2559 & 0.214 \\
TD-byShort-1024 & 0.2608 & 0.257 \\
AMNet & 0.159 & 0.194 \\
TD-2048 (from Short-term) & 0.5325 (on Short) & 0.251 \\
TD-1024 (from Short-term) & 0.5105 (on Short) & 0.253
\end{tabular}
\caption{Long-term Spearman's Rank Correlation (Official Test Set)}
\end{center}
\end{table}

The rank correlation score for models which were trained/validated on short-term dataset do have better results than those trained on long-term dateset.